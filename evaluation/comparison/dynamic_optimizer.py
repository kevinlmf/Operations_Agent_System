"""
Dynamic Environment Optimizer - é’ˆå¯¹åŠ¨æ€ç¯å¢ƒä¼˜åŒ–RL/DLæ–¹æ³•

ç›®æ ‡ï¼šè®©RLå’ŒDLæ–¹æ³•åœ¨åŠ¨æ€ç¯å¢ƒä¸‹beatä¼ ç»Ÿbaseline
ç­–ç•¥ï¼š
1. è¶…å‚æ•°ä¼˜åŒ–ï¼ˆé’ˆå¯¹åŠ¨æ€åœºæ™¯ï¼‰
2. å­£èŠ‚æ€§é€‚åº”
3. è¶‹åŠ¿é€‚åº”
4. ä¸ç¡®å®šæ€§å¤„ç†
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
from scipy.optimize import differential_evolution, minimize
import sys
from pathlib import Path
import time

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from goal.interfaces import InventoryMethod, InventoryState
from evaluation.comparison.dynamic_scenario_evaluator import DynamicScenarioEvaluator, ScenarioCharacteristics
from evaluation.comparison.net_benefit_optimizer import NetBenefitOptimizer


class DynamicOptimizer:
    """
    åŠ¨æ€ç¯å¢ƒä¼˜åŒ–å™¨
    
    ä¸“é—¨é’ˆå¯¹åŠ¨æ€åœºæ™¯ï¼ˆå­£èŠ‚æ€§ã€è¶‹åŠ¿ã€ä¸ç¡®å®šæ€§ï¼‰ä¼˜åŒ–RL/DLæ–¹æ³•
    """
    
    def __init__(self,
                 base_optimizer: NetBenefitOptimizer,
                 scenario: ScenarioCharacteristics,
                 baseline_method: InventoryMethod,
                 baseline_performance: float):
        """
        åˆå§‹åŒ–åŠ¨æ€ä¼˜åŒ–å™¨
        
        Args:
            base_optimizer: Net Benefitä¼˜åŒ–å™¨
            scenario: åŠ¨æ€åœºæ™¯ç‰¹å¾
            baseline_method: baselineæ–¹æ³•ï¼ˆé€šå¸¸æ˜¯EOQï¼‰
            baseline_performance: baselineçš„Net Benefit
        """
        self.base_optimizer = base_optimizer
        self.scenario = scenario
        self.baseline_method = baseline_method
        self.baseline_performance = baseline_performance
        self.dynamic_evaluator = DynamicScenarioEvaluator(base_optimizer, scenario)
    
    def optimize_dqn_for_dynamic(self,
                                 train_demand: np.ndarray,
                                 test_states: List[InventoryState],
                                 test_demands: np.ndarray,
                                 num_scenarios: int = 10) -> Dict[str, Any]:
        """
        é’ˆå¯¹åŠ¨æ€ç¯å¢ƒä¼˜åŒ–DQN
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. è°ƒæ•´ç½‘ç»œç»“æ„ï¼ˆé€‚åº”åŠ¨æ€æ¨¡å¼ï¼‰
        2. è°ƒæ•´å­¦ä¹ ç‡ï¼ˆé€‚åº”ä¸ç¡®å®šæ€§ï¼‰
        3. è°ƒæ•´explorationç­–ç•¥ï¼ˆé€‚åº”å­£èŠ‚æ€§ï¼‰
        4. å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆé€‚åº”å¤æ‚æ¨¡å¼ï¼‰
        """
        from methods.rl_methods.dqn import DQNInventoryMethod
        
        print("  ğŸ”§ ä¼˜åŒ–DQNä»¥beat baseline...")
        print(f"     Baseline Net Benefit: ${self.baseline_performance:,.2f}")
        
        def objective(params):
            """ä¼˜åŒ–ç›®æ ‡ï¼šæœ€å¤§åŒ–é£é™©è°ƒæ•´Net Benefit"""
            lr, hidden, episodes, epsilon_decay = params
            hidden = int(max(hidden, 16))  # è‡³å°‘16
            episodes = int(max(episodes, 5))  # è‡³å°‘5
            
            try:
                dqn = DQNInventoryMethod(
                    state_dim=6,
                    num_actions=21,
                    hidden_sizes=(hidden, hidden),
                    learning_rate=max(lr, 0.0001),
                    memory_size=10000,
                    batch_size=64,
                    epsilon_decay=max(epsilon_decay, 0.9)
                )
                dqn.fit(train_demand)
                dqn.train_agent(num_episodes=int(episodes), fast_mode=True)
                
                # è¯„ä¼°åœ¨åŠ¨æ€åœºæ™¯ä¸‹çš„è¡¨ç°
                result = self.dynamic_evaluator.evaluate_method_comprehensive(
                    dqn, train_demand, test_states, test_demands, num_scenarios=5  # å‡å°‘åœºæ™¯æ•°åŠ å¿«ä¼˜åŒ–
                )
                
                # ç›®æ ‡ï¼šæœ€å¤§åŒ–é£é™©è°ƒæ•´Net Benefit
                risk_adj_nb = result.get('risk_adjusted_net_benefit', -1e6)
                
                # å¦‚æœè¶…è¿‡baselineï¼Œç»™äºˆå¥–åŠ±
                if risk_adj_nb > self.baseline_performance:
                    return -risk_adj_nb * 0.9  # å¥–åŠ±ï¼šè¶…è¿‡baselineæ—¶é™ä½æƒ©ç½š
                else:
                    return -risk_adj_nb  # æƒ©ç½šï¼šæœªè¶…è¿‡baseline
                    
            except Exception as e:
                return 1e6  # æƒ©ç½šæ— æ•ˆå‚æ•°
        
        # å‚æ•°èŒƒå›´
        bounds = [
            (0.0001, 0.01),      # learning_rate
            (32, 128),           # hidden_size
            (10, 50),            # episodes
            (0.9, 0.999)         # epsilon_decay
        ]
        
        # ä½¿ç”¨å·®åˆ†è¿›åŒ–ç®—æ³•
        print("     ğŸ” æœç´¢æœ€ä½³å‚æ•°...")
        result = differential_evolution(
            objective,
            bounds,
            seed=42,
            maxiter=15,  # å‡å°‘è¿­ä»£æ¬¡æ•°ï¼ˆRLè®­ç»ƒæ…¢ï¼‰
            popsize=5,
            atol=1e-4,
            polish=False  # ä¸è¿›è¡Œå±€éƒ¨ä¼˜åŒ–ï¼ˆåŠ å¿«é€Ÿåº¦ï¼‰
        )
        
        best_lr, best_hidden, best_episodes, best_eps_decay = result.x
        best_hidden = int(max(best_hidden, 16))
        best_episodes = int(max(best_episodes, 10))
        
        print(f"     âœ… æ‰¾åˆ°æœ€ä½³å‚æ•°:")
        print(f"        Learning Rate: {best_lr:.6f}")
        print(f"        Hidden Size: {best_hidden}")
        print(f"        Episodes: {best_episodes}")
        print(f"        Epsilon Decay: {best_eps_decay:.4f}")
        
        # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒå®Œæ•´æ¨¡å‹
        print("     ğŸ¯ ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒå®Œæ•´æ¨¡å‹...")
        best_dqn = DQNInventoryMethod(
            state_dim=6,
            num_actions=21,
            hidden_sizes=(best_hidden, best_hidden),
            learning_rate=best_lr,
            memory_size=10000,
            batch_size=64,
            epsilon_decay=best_eps_decay
        )
        best_dqn.fit(train_demand)
        best_dqn.train_agent(num_episodes=best_episodes, fast_mode=False)  # å®Œæ•´è®­ç»ƒ
        
        # æœ€ç»ˆè¯„ä¼°
        final_result = self.dynamic_evaluator.evaluate_method_comprehensive(
            best_dqn, train_demand, test_states, test_demands, num_scenarios
        )
        
        return {
            'method': best_dqn,
            'parameters': {
                'learning_rate': best_lr,
                'hidden_size': best_hidden,
                'episodes': best_episodes,
                'epsilon_decay': best_eps_decay
            },
            'performance': final_result,
            'beats_baseline': final_result.get('risk_adjusted_net_benefit', 0) > self.baseline_performance
        }
    
    def optimize_lstm_for_dynamic(self,
                                 train_demand: np.ndarray,
                                 test_states: List[InventoryState],
                                 test_demands: np.ndarray,
                                 num_scenarios: int = 10) -> Dict[str, Any]:
        """
        é’ˆå¯¹åŠ¨æ€ç¯å¢ƒä¼˜åŒ–LSTM
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. è°ƒæ•´åºåˆ—é•¿åº¦ï¼ˆé€‚åº”å­£èŠ‚æ€§å‘¨æœŸï¼‰
        2. è°ƒæ•´ç½‘ç»œç»“æ„ï¼ˆé€‚åº”å¤æ‚æ¨¡å¼ï¼‰
        3. è°ƒæ•´è®­ç»ƒè½®æ•°ï¼ˆé€‚åº”è¶‹åŠ¿å˜åŒ–ï¼‰
        4. æ·»åŠ å­£èŠ‚æ€§ç‰¹å¾
        """
        from methods.ml_methods.lstm import LSTMInventoryMethod
        
        print("  ğŸ”§ ä¼˜åŒ–LSTMä»¥beat baseline...")
        print(f"     Baseline Net Benefit: ${self.baseline_performance:,.2f}")
        
        def objective(params):
            """ä¼˜åŒ–ç›®æ ‡ï¼šæœ€å¤§åŒ–é£é™©è°ƒæ•´Net Benefit"""
            seq_len, hidden, epochs = params
            seq_len = int(max(seq_len, 7))  # è‡³å°‘7å¤©ï¼ˆä¸€å‘¨ï¼‰
            hidden = int(max(hidden, 16))
            epochs = int(max(epochs, 5))
            
            try:
                lstm = LSTMInventoryMethod(
                    sequence_length=seq_len,
                    hidden_size=hidden,
                    num_layers=1,
                    epochs=epochs,
                    batch_size=64
                )
                lstm.fit(train_demand)
                
                # è¯„ä¼°åœ¨åŠ¨æ€åœºæ™¯ä¸‹çš„è¡¨ç°
                result = self.dynamic_evaluator.evaluate_method_comprehensive(
                    lstm, train_demand, test_states, test_demands, num_scenarios=5
                )
                
                risk_adj_nb = result.get('risk_adjusted_net_benefit', -1e6)
                
                # å¦‚æœè¶…è¿‡baselineï¼Œç»™äºˆå¥–åŠ±
                if risk_adj_nb > self.baseline_performance:
                    return -risk_adj_nb * 0.9
                else:
                    return -risk_adj_nb
                    
            except Exception as e:
                return 1e6
        
        # å‚æ•°èŒƒå›´ï¼ˆé’ˆå¯¹åŠ¨æ€åœºæ™¯ä¼˜åŒ–ï¼‰
        bounds = [
            (7, 60),    # sequence_lengthï¼ˆé€‚åº”å‘¨å’Œæœˆå­£èŠ‚æ€§ï¼‰
            (16, 128),  # hidden_size
            (10, 30)    # epochs
        ]
        
        print("     ğŸ” æœç´¢æœ€ä½³å‚æ•°...")
        result = differential_evolution(
            objective,
            bounds,
            seed=42,
            maxiter=20,
            popsize=5,
            atol=1e-4
        )
        
        best_seq_len, best_hidden, best_epochs = result.x
        best_seq_len = int(max(best_seq_len, 7))
        best_hidden = int(max(best_hidden, 16))
        best_epochs = int(max(best_epochs, 10))
        
        print(f"     âœ… æ‰¾åˆ°æœ€ä½³å‚æ•°:")
        print(f"        Sequence Length: {best_seq_len}")
        print(f"        Hidden Size: {best_hidden}")
        print(f"        Epochs: {best_epochs}")
        
        # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒå®Œæ•´æ¨¡å‹
        print("     ğŸ¯ ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒå®Œæ•´æ¨¡å‹...")
        best_lstm = LSTMInventoryMethod(
            sequence_length=best_seq_len,
            hidden_size=best_hidden,
            num_layers=1,
            epochs=best_epochs,
            batch_size=64
        )
        best_lstm.fit(train_demand)
        
        # æœ€ç»ˆè¯„ä¼°
        final_result = self.dynamic_evaluator.evaluate_method_comprehensive(
            best_lstm, train_demand, test_states, test_demands, num_scenarios
        )
        
        return {
            'method': best_lstm,
            'parameters': {
                'sequence_length': best_seq_len,
                'hidden_size': best_hidden,
                'epochs': best_epochs
            },
            'performance': final_result,
            'beats_baseline': final_result.get('risk_adjusted_net_benefit', 0) > self.baseline_performance
        }
    
    def add_seasonal_features_to_state(self, state: InventoryState) -> np.ndarray:
        """
        ä¸ºçŠ¶æ€æ·»åŠ å­£èŠ‚æ€§ç‰¹å¾ï¼ˆå¸®åŠ©RL/DLæ–¹æ³•é€‚åº”å­£èŠ‚æ€§ï¼‰
        
        æ·»åŠ ç‰¹å¾ï¼š
        - å¹´åº¦å­£èŠ‚æ€§ï¼ˆsin/cosï¼‰
        - å‘¨å­£èŠ‚æ€§ï¼ˆsin/cosï¼‰
        - è¶‹åŠ¿æŒ‡æ ‡
        """
        t = state.time_step
        
        # å¹´åº¦å­£èŠ‚æ€§ç‰¹å¾
        annual_sin = np.sin(2 * np.pi * t / 365.25)
        annual_cos = np.cos(2 * np.pi * t / 365.25)
        
        # å‘¨å­£èŠ‚æ€§ç‰¹å¾
        weekly_sin = np.sin(2 * np.pi * t / 7)
        weekly_cos = np.cos(2 * np.pi * t / 7)
        
        # è¶‹åŠ¿ç‰¹å¾ï¼ˆåŸºäºå†å²éœ€æ±‚ï¼‰
        if len(state.demand_history) >= 7:
            recent_mean = np.mean(state.demand_history[-7:])
            older_mean = np.mean(state.demand_history[-30:-7]) if len(state.demand_history) >= 30 else recent_mean
            trend = (recent_mean - older_mean) / (older_mean + 1e-6)
        else:
            trend = 0.0
        
        return np.array([annual_sin, annual_cos, weekly_sin, weekly_cos, trend])
    
    def compare_with_baseline(self,
                             optimized_methods: Dict[str, Dict[str, Any]],
                             test_states: List[InventoryState],
                             test_demands: np.ndarray) -> pd.DataFrame:
        """
        å¯¹æ¯”ä¼˜åŒ–åçš„æ–¹æ³•ä¸baseline
        """
        results = []
        
        # Baselineç»“æœ
        baseline_result = self.dynamic_evaluator.evaluate_method_comprehensive(
            self.baseline_method,
            test_states[0].demand_history if test_states else np.array([]),
            test_states,
            test_demands,
            num_scenarios=10
        )
        
        results.append({
            'method_name': 'Baseline (EOQ)',
            'is_baseline': True,
            'risk_adjusted_net_benefit': baseline_result.get('risk_adjusted_net_benefit', 0),
            'expected_net_benefit': baseline_result.get('expected_net_benefit', 0),
            'risk': baseline_result.get('risk', 0),
            'forecast_accuracy': baseline_result.get('forecast_accuracy', 0),
            'improvement_pct': 0.0
        })
        
        # ä¼˜åŒ–åçš„æ–¹æ³•
        for method_name, opt_result in optimized_methods.items():
            perf = opt_result['performance']
            risk_adj_nb = perf.get('risk_adjusted_net_benefit', 0)
            improvement = ((risk_adj_nb - self.baseline_performance) / abs(self.baseline_performance)) * 100
            
            results.append({
                'method_name': f"{method_name}_Optimized",
                'is_baseline': False,
                'risk_adjusted_net_benefit': risk_adj_nb,
                'expected_net_benefit': perf.get('expected_net_benefit', 0),
                'risk': perf.get('risk', 0),
                'forecast_accuracy': perf.get('forecast_accuracy', 0),
                'improvement_pct': improvement,
                'beats_baseline': opt_result.get('beats_baseline', False),
                'parameters': opt_result.get('parameters', {})
            })
        
        df = pd.DataFrame(results)
        df = df.sort_values('risk_adjusted_net_benefit', ascending=False)
        
        return df




