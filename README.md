# Operations Agent System Â· AI-Driven Inventory Optimization Platform

**Multi-Agent Decision System** powered by Reinforcement Learning and Operations Research, combining dynamic scenario evaluation, net benefit optimization, and hierarchical planning to deliver automated inventory management for supply chain operations.

---

[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.8+-green.svg)](https://python.org)
[![JAX](https://img.shields.io/badge/JAX-Accelerated-orange.svg)](https://github.com/google/jax)

---

## Project Positioning

**AI-Driven Inventory Optimization** - A research platform that combines:

- **Operations Research**: Multi-level optimization (MIP/LP/DP), EOQ, Safety Stock, dynamic programming
- **AI/ML Engineering**: Multi-agent RL (DQN, SAC), deep learning (JAX/Flax), LSTM forecasting
- **Research Innovation**: Goal-oriented planning, net benefit optimization, regime-adaptive systems

## Key Features

### 1. Unified Method Interface

- **Traditional Methods**: EOQ, Safety Stock, (s,S) Policy
- **ML Methods**: LSTM, Transformer for demand forecasting
- **RL Methods**: DQN, Multi-Agent RL for adaptive decisions
- **Action Composition**: Unified `InventoryMethod` interface

### 2. Hierarchical Multi-Agent System

- **Strategic Agent (MIP)**: Facility location, network design
- **Tactical Agent (LP)**: Inventory allocation, capacity planning
- **Operational Agent (DP)**: Daily ordering, replenishment

### 3. Net Benefit Optimization

- **Objective**: Maximize Net Benefit = Revenue - Total Cost
- **Cost Components**: Holding, stockout, ordering, implementation
- **ROI Analysis**: Return on investment tracking

### 4. Dynamic Scenario Evaluation

- **Seasonality Detection**: Annual and weekly patterns
- **Trend Analysis**: Upward, downward, stable trends
- **Uncertainty Quantification**: CV coefficient for volatility
- **Monte Carlo Simulation**: Multi-scenario risk assessment

### 5. Risk-Adjusted Performance

- **Expected Net Benefit**: Mean across scenarios
- **Risk Measurement**: Standard deviation
- **Risk-Adjusted Metric**: Expected Return - 0.5 Ã— Risk


## Installation

```bash
# Clone the repository
git clone https://github.com/kevinlmf/Operations_Agent_System
cd Operations_Agent_System

# Create and activate virtual environment
python -m venv venv
source venv/bin/activate        # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

## Quick Start

### Run Dynamic Scenario Evaluation

Evaluate methods across dynamic scenarios with seasonality, trends, and uncertainty:

```bash
python evaluate_dynamic_scenarios.py
```

### Run Net Benefit Optimization

Find optimal method maximizing Net Benefit = Revenue - Total Cost:

```bash
python evaluate_net_benefit.py
```

### Optimize RL/DL to Beat Baseline

Automatically optimize RL/DL parameters to outperform traditional methods:

```bash
python optimize_for_dynamic.py
```

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Goal Layer                                                   â”‚
â”‚  Input: U(net_benefit, cost, risk, service_level)            â”‚
â”‚  Output: Goal directive (order quantities, reorder points)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scenario Detector + Dynamic Evaluator                       â”‚
â”‚  Seasonality | Trend | Uncertainty Detection                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Net Benefit Optimizer                                        â”‚
â”‚  Maximize: Revenue - (Holding + Stockout + Ordering + Impl)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚              â”‚              â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ Strategic â”‚  â”‚ Tactical  â”‚  â”‚Operationalâ”‚
â”‚   Agent   â”‚  â”‚   Agent   â”‚  â”‚   Agent   â”‚
â”‚   (MIP)   â”‚  â”‚   (LP)    â”‚  â”‚   (DP)    â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â”‚              â”‚              â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Method Selector             â”‚
      â”‚  Traditional | ML | RL       â”‚
      â”‚  EOQ | LSTM | DQN | SAC     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
Operations_Agent_System/
â”œâ”€â”€ goal/                          # Goal definitions & interfaces
â”‚   â”œâ”€â”€ interfaces.py             # InventoryMethod interface
â”‚   â”œâ”€â”€ objective_selector.py     # Goal selection logic
â”‚   â””â”€â”€ mathematical_framework.py # Mathematical models
â”‚
â”œâ”€â”€ agent/                         # All agent implementations
â”‚   â”œâ”€â”€ traditional/              # Traditional methods
â”‚   â”‚   â”œâ”€â”€ eoq.py               # Economic Order Quantity
â”‚   â”‚   â”œâ”€â”€ safety_stock.py      # Safety Stock method
â”‚   â”‚   â””â”€â”€ s_S_policy.py        # (s,S) Policy
â”‚   â”œâ”€â”€ ml_methods/               # ML methods
â”‚   â”‚   â”œâ”€â”€ lstm.py              # LSTM forecasting
â”‚   â”‚   â””â”€â”€ transformer.py       # Transformer model
â”‚   â”œâ”€â”€ rl_methods/               # RL methods
â”‚   â”‚   â”œâ”€â”€ dqn.py               # Deep Q-Network
â”‚   â”‚   â””â”€â”€ multi_agent.py       # Multi-agent RL
â”‚   â”œâ”€â”€ orchestrator/             # Multi-agent orchestration
â”‚   â”‚   â”œâ”€â”€ orchestrator.py      # Main orchestrator
â”‚   â”‚   â””â”€â”€ definitions.py       # Context definitions
â”‚   â”œâ”€â”€ environment/              # RL environment
â”‚   â”‚   â””â”€â”€ env.py               # Gym-compatible environment
â”‚   â””â”€â”€ or_optimization/          # OR optimizers
â”‚       â”œâ”€â”€ linear_programming.py
â”‚       â”œâ”€â”€ mixed_integer_programming.py
â”‚       â””â”€â”€ dynamic_programming.py
â”‚
â”œâ”€â”€ evaluation/                    # Evaluation framework
â”‚   â”œâ”€â”€ comparison/               # Method comparison
â”‚   â”‚   â”œâ”€â”€ evaluator.py         # Basic evaluator
â”‚   â”‚   â”œâ”€â”€ net_benefit_optimizer.py    # Net Benefit optimization
â”‚   â”‚   â”œâ”€â”€ dynamic_scenario_evaluator.py  # Dynamic evaluation
â”‚   â”‚   â”œâ”€â”€ dynamic_optimizer.py # RL/DL optimizer
â”‚   â”‚   â””â”€â”€ parameter_optimizer.py # Parameter tuning
â”‚   â””â”€â”€ risk_management/          # Risk control
â”‚       â”œâ”€â”€ anomaly_detector.py
â”‚       â””â”€â”€ contingency_planner.py
â”‚
â”œâ”€â”€ results/                       # Output folder (CSV, PNG)
â”‚
â”œâ”€â”€ evaluate_dynamic_scenarios.py  # Dynamic scenario evaluation
â”œâ”€â”€ evaluate_net_benefit.py        # Net Benefit optimization
â”œâ”€â”€ evaluate_system.py             # Basic system evaluation
â”œâ”€â”€ optimize_for_dynamic.py        # RL/DL optimization
â””â”€â”€ README.md
```


## Execution Flow

```
1. Data Generation / Loading
   â†“
2. Scenario Detection (Seasonality, Trend, Uncertainty)
   â†“
3. Method Training (Traditional, ML, RL)
   â†“
4. Net Benefit Evaluation
   â†“
5. Risk-Adjusted Comparison
   â†“
6. Optimal Method Selection
```

## Performance Optimization

- **JAX Acceleration**: All ML/RL methods use JAX/Flax
- **Fast Mode**: `fast_mode=True` reduces training time
- **JIT Compilation**: Key functions use `@jit` decorator

## Experimental Results

Based on comprehensive evaluations across multiple scenarios with 90-day test periods and 365-day training data:

### Net Benefit Analysis

| Method | Category | Net Benefit | ROI | Service Level | Forecast Acc |
|--------|----------|-------------|-----|---------------|--------------|
| **Safety Stock** | Traditional | **$78,452.64** | **7,745%** | **76.67%** | 81.03% |
| EOQ | Traditional | -$11,427.55 | -1,243% | 44.44% | 81.03% |
| LSTM | ML | -$59,156.53 | -945% | 1.11% | 78.87% |
| DQN | RL | -$74,092.00 | -670% | 1.11% | 82.28% |

### Cost Breakdown

| Method | Operational Cost | Implementation | Training | Inference | Maintenance | Total Cost |
|--------|------------------|----------------|----------|-----------|-------------|------------|
| Safety Stock | $15,498.36 | $1,000 | $0 | $9 | $900 | $17,407.36 |
| EOQ | $11,518.55 | $1,000 | $0 | $9 | $900 | $13,427.55 |
| LSTM | $49,611.53 | $5,000 | $2,000 | $45 | $4,500 | $61,156.53 |
| DQN | $54,002.00 | $8,000 | $5,000 | $90 | $9,000 | $76,092.00 |

### Dynamic Scenario Performance

Evaluation under dynamic conditions with seasonality, trends, and 20% uncertainty:

- **Seasonality Detection**: Successfully identified annual (365.25-day) and weekly (7-day) patterns
- **Trend Adaptation**: Detected upward/downward trends with 5% strength
- **Uncertainty Handling**: Monte Carlo simulation with 10 scenarios for risk assessment
- **Risk-Adjusted Metric**: Expected Return - 0.5 Ã— Risk for conservative optimization

### Key Findings

1. **Traditional Methods Dominate**: Safety Stock achieves highest Net Benefit with proper service level management
2. **RL/ML Training Gap**: DQN and LSTM require more training episodes to compete with traditional baselines
3. **Cost-Benefit Trade-off**: Higher implementation costs of ML/RL methods not justified by current performance
4. **Forecast Accuracy**: All methods achieve ~80% forecast accuracy, but action decisions vary significantly
5. **Service Level Critical**: High service level (76.67%) directly correlates with Net Benefit maximization

## Future Work: Making ML/DL Beat Traditional Methods

The current results show traditional methods (Safety Stock) significantly outperforming ML/DL approaches. Here's the roadmap to close this gap:

### 1. Training Scale & Stability

- **Increase Episodes**: Scale from 10 â†’ 500+ episodes for proper policy convergence
- **Curriculum Learning**: Start with stable demand, progressively add seasonality and uncertainty
- **Reward Shaping**: Design rewards that explicitly penalize stockouts and incentivize service levels
- **Pre-training**: Use imitation learning to warm-start RL agents from Safety Stock policy

### 2. State Representation & Feature Engineering

- **Temporal Features**: Add day-of-week, month-of-year, holiday indicators to capture seasonality
- **Trend Signals**: Include moving averages (7-day, 30-day) and momentum indicators
- **Inventory Context**: Encode days-of-supply, stockout history, and order pipeline status
- **Attention Mechanism**: Let LSTM/Transformer learn which historical patterns matter most

### 3. Action Space & Policy Design

- **Continuous Actions**: Replace discrete order quantities with continuous policy (SAC/PPO)
- **Action Bounds**: Constrain actions to feasible ranges based on EOQ/Safety Stock baselines
- **Hybrid Policy**: Combine OR-computed baseline with RL-learned adjustments (residual RL)
- **Multi-step Planning**: Use model-based RL to plan ahead during high-uncertainty periods

### 4. Objective Alignment

- **Service Level Constraint**: Add hard constraint for minimum 95% service level
- **Cost-Aware Reward**: Include holding/stockout costs directly in reward function
- **Risk-Sensitive RL**: Use CVaR or worst-case optimization for robust policies
- **Multi-Objective**: Pareto optimization balancing cost, service, and inventory turnover

### 5. Evaluation & Deployment

- **Longer Test Horizons**: Evaluate over 365+ days to capture full seasonal cycles
- **Out-of-Distribution Testing**: Test on demand patterns not seen during training
- **Ensemble Methods**: Combine predictions from LSTM + DQN + Traditional for robustness
- **Online Adaptation**: Implement continuous learning to adapt to demand distribution shifts



## License

This project is licensed under the MIT License.

## Disclaimer

 **Important**: This project is provided for educational, academic research, and learning purposes only. The inventory decisions generated by this system should be validated by domain experts before implementation in production environments.

---

May our lives keep optimizing, like finding balance in every stepğŸ˜Š
